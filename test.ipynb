{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1522e38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "import json\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e3d201",
   "metadata": {},
   "source": [
    "# JSON to CSV Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "610950cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_edges_to_csv(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Parse edges.jsonl and extract subject, predicate, and object columns to CSV\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r') as jsonl_file, open(output_file, 'w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        \n",
    "        # Write header\n",
    "        writer.writerow(['subject', 'predicate', 'object'])\n",
    "        \n",
    "        # Process each line in the JSONL file\n",
    "        for line in jsonl_file:\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                subject = data.get('subject', '')\n",
    "                predicate = data.get('predicate', '')\n",
    "                obj = data.get('object', '')\n",
    "                \n",
    "                writer.writerow([subject, predicate, obj])\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Skipping invalid JSON line: {line}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccf30118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created: edges_output.csv\n"
     ]
    }
   ],
   "source": [
    "input_file = \"example_edges.jsonl\"\n",
    "output_file = \"edges_output.csv\"\n",
    "    \n",
    "parse_edges_to_csv(input_file, output_file)\n",
    "print(f\"CSV file created: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbaa0ca",
   "metadata": {},
   "source": [
    "# Sub graph Preparation (random predicate removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15ff831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_chunk_and_remove_predicates(input_csv, chunk_size=100, predicate_removal_percent=50, output_file='modified_chunk.csv'):\n",
    "    \"\"\"\n",
    "    Select a random chunk from the CSV and remove a percentage of edges for each unique predicate.\n",
    "    \n",
    "    Args:\n",
    "        input_csv: Path to the input CSV file\n",
    "        chunk_size: Number of rows to select (default: 100)\n",
    "        predicate_removal_percent: Percentage of edges to remove for each unique predicate (default: 50)\n",
    "        output_file: Path to save the modified chunk\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (original_chunk_df, modified_chunk_df)\n",
    "    \"\"\"\n",
    "    # Read the CSV\n",
    "    df = pd.read_csv(input_csv)\n",
    "    \n",
    "    # Select a random chunk\n",
    "    if chunk_size >= len(df):\n",
    "        chunk_df = df.copy()\n",
    "    else:\n",
    "        start_idx = random.randint(0, len(df) - chunk_size)\n",
    "        chunk_df = df.iloc[start_idx:start_idx + chunk_size].copy()\n",
    "    \n",
    "    # Store original chunk\n",
    "    original_chunk = chunk_df.copy()\n",
    "    modified_chunk = chunk_df.copy()\n",
    "    \n",
    "    # Get unique predicates in the chunk\n",
    "    unique_predicates = modified_chunk['predicate'].unique()\n",
    "    \n",
    "    total_removed = 0\n",
    "    \n",
    "    # Remove specified percentage of edges for each unique predicate\n",
    "    for predicate in unique_predicates:\n",
    "        predicate_indices = modified_chunk[modified_chunk['predicate'] == predicate].index.tolist()\n",
    "        num_to_remove_pred = int(len(predicate_indices) * (predicate_removal_percent / 100))\n",
    "        \n",
    "        if num_to_remove_pred > 0:\n",
    "            indices_to_remove_pred = random.sample(predicate_indices, num_to_remove_pred)\n",
    "            modified_chunk.loc[indices_to_remove_pred, 'predicate'] = ''\n",
    "            total_removed += num_to_remove_pred\n",
    "    \n",
    "    # Save modified chunk to CSV\n",
    "    modified_chunk.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Original chunk size: {len(original_chunk)}\")\n",
    "    print(f\"Removed {predicate_removal_percent}% of edges for each unique predicate\")\n",
    "    print(f\"Total edges with predicates removed: {total_removed}\")\n",
    "    print(f\"Modified chunk size: {len(modified_chunk)}\")\n",
    "    print(f\"Modified chunk saved to: {output_file}\")\n",
    "    \n",
    "    return original_chunk, modified_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a9d9169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original chunk size: 100\n",
      "Removed 50% of edges for each unique predicate\n",
      "Total edges with predicates removed: 42\n",
      "Modified chunk size: 100\n",
      "Modified chunk saved to: modified_chunk_50%_removed.csv\n"
     ]
    }
   ],
   "source": [
    "original, modified = select_chunk_and_remove_predicates(\n",
    "    'edges_output.csv',\n",
    "    chunk_size=100,\n",
    "    predicate_removal_percent=50,\n",
    "    output_file='modified_chunk_50%_removed.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ae98e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Chunk:\n",
      "             subject          predicate         object\n",
      "1322  UBERON:0003092   biolink:has_part   PR:000027222\n",
      "1323     CHEBI:39867     biolink:causes  UMLS:C1868980\n",
      "1324     CHEBI:39867     biolink:causes  UMLS:C1868980\n",
      "1325      GO:0004714  biolink:regulates   NCBIGene:207\n",
      "1326   UMLS:C0023775   biolink:disrupts     GO:0006811\n",
      "\n",
      "Modified Chunk:\n",
      "             subject         predicate         object\n",
      "1322  UBERON:0003092  biolink:has_part   PR:000027222\n",
      "1323     CHEBI:39867                    UMLS:C1868980\n",
      "1324     CHEBI:39867                    UMLS:C1868980\n",
      "1325      GO:0004714                     NCBIGene:207\n",
      "1326   UMLS:C0023775  biolink:disrupts     GO:0006811\n",
      "\n",
      "Removed Rows:\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Chunk:\")\n",
    "print(original.head())\n",
    "print(\"\\nModified Chunk:\")\n",
    "print(modified.head())\n",
    "print(\"\\nRemoved Rows:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff92acce",
   "metadata": {},
   "source": [
    "# Random Edge Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d1a251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to randomly assign edges to nodes from the list of unique predicates \n",
    "def randomly_assign_edges(input_csv, unique_predicates, output_file='randomly_assigned_edges.csv'):\n",
    "    \"\"\"\n",
    "    Randomly assign edges to nodes from the list of unique predicates and save to new CSV.\n",
    "    \n",
    "    Args:\n",
    "        input_csv: Path to the input CSV file\n",
    "        unique_predicates: List of unique predicates\n",
    "        output_file: Path to save the new CSV with randomly assigned edges\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "    \n",
    "    # Fill empty predicates with random choices from unique_predicates\n",
    "    for idx, row in df.iterrows():\n",
    "        if pd.isna(row['predicate']) or row['predicate'] == '' or str(row['predicate']).strip() == '':\n",
    "            df.at[idx, 'predicate'] = random.choice(unique_predicates)\n",
    "    \n",
    "    # Save to new CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Randomly assigned edges saved to: {output_file}\")\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ace49410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique predicates found:\n",
      "['biolink:has_part' 'biolink:disrupts' 'biolink:correlated_with'\n",
      " 'biolink:positively_correlated_with' 'biolink:causes' 'biolink:regulates'\n",
      " 'biolink:coexists_with' 'biolink:located_in' 'biolink:contraindicated_in'\n",
      " 'biolink:capable_of' 'biolink:negatively_correlated_with'\n",
      " 'biolink:occurs_in' 'biolink:develops_from'\n",
      " 'biolink:has_active_ingredient'\n",
      " 'biolink:treats_or_applied_or_studied_to_treat' 'biolink:affects'\n",
      " 'biolink:increases_response_to' 'biolink:genetically_associated_with'\n",
      " 'biolink:overlaps' 'biolink:predisposes_to_condition'\n",
      " 'biolink:expressed_in' 'biolink:ameliorates_condition'\n",
      " 'biolink:contributes_to' 'biolink:decreases_response_to' 'biolink:treats'\n",
      " 'biolink:subclass_of' 'biolink:similar_to' 'biolink:has_participant']\n",
      "28\n",
      "Randomly assigned edges saved to: randomly_assigned_edges.csv\n"
     ]
    }
   ],
   "source": [
    "# list of all unique predicates in the dataset which are not empty\n",
    "unique_predicates = modified['predicate'].unique()\n",
    "unique_predicates = unique_predicates[unique_predicates != '']\n",
    "print(\"Unique predicates found:\")\n",
    "print(unique_predicates)\n",
    "print(len(unique_predicates))\n",
    "\n",
    "# Run the improved function\n",
    "result_df = randomly_assign_edges('modified_chunk_50%_removed.csv', unique_predicates, output_file='randomly_assigned_edges.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51826422",
   "metadata": {},
   "source": [
    "# Gemini LLM Edge Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33b6ff5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/smohanty13/Desktop/Hackathon/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "api = \"AIzaSyCQVqiw_JyVbMrko4TpplqS0bf2GJCtgr8\"\n",
    "\n",
    "def fill_missing_predicates_llm_base(input_df, unique_predicates, output_file='llm_filled_predicates.csv', \n",
    "                                    metrics_file='llm_metrics.json', responses_file='llm_responses.json'):\n",
    "    \"\"\"\n",
    "    Use Gemini API to fill in missing predicates in the DataFrame using a single batch prompt.\n",
    "    \n",
    "    Args:\n",
    "        input_df: DataFrame with potential missing predicates\n",
    "        unique_predicates: List of unique predicates to choose from\n",
    "        output_file: Path to save the new CSV with LLM filled predicates\n",
    "        metrics_file: Path to save metrics about the LLM usage\n",
    "        responses_file: Path to save all LLM responses for analysis\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (filled_df, metrics, responses)\n",
    "    \"\"\"\n",
    "    # Configure Gemini API\n",
    "    genai.configure(api_key=api)\n",
    "    model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "    \n",
    "    df = input_df.copy()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Find all empty predicate rows\n",
    "    empty_mask = df['predicate'].isna() | (df['predicate'] == '') | (df['predicate'].str.strip() == '')\n",
    "    empty_indices = df[empty_mask].index.tolist()\n",
    "    empty_count = len(empty_indices)\n",
    "    \n",
    "    print(f\"Found {empty_count} empty predicates to fill\")\n",
    "    \n",
    "    if empty_count == 0:\n",
    "        print(\"No empty predicates found!\")\n",
    "        return df, {}, []\n",
    "    \n",
    "    # Build single large prompt with all missing predicates\n",
    "    predicate_list = ', '.join(unique_predicates)\n",
    "    \n",
    "    batch_prompt = f\"\"\"You are a biomedical knowledge graph expert. Complete the missing predicates for these triples.\n",
    "\n",
    "Available predicates: {predicate_list}\n",
    "\n",
    "Instructions: For each numbered triple, respond with ONLY the most appropriate predicate from the list above.\n",
    "\n",
    "Triples to complete:\n",
    "\"\"\"\n",
    "    \n",
    "    # Add all empty predicate cases to the prompt\n",
    "    case_mapping = {}  # Maps case number to dataframe index\n",
    "    for case_num, idx in enumerate(empty_indices, 1):\n",
    "        row = df.iloc[idx]\n",
    "        batch_prompt += f\"{case_num}. Subject: {row['subject']} | Object: {row['object']}\\n\"\n",
    "        case_mapping[case_num] = idx\n",
    "    \n",
    "    batch_prompt += f\"\"\"\n",
    "Expected response format:\n",
    "1. predicate_name\n",
    "2. predicate_name\n",
    "3. predicate_name\n",
    "...\n",
    "\n",
    "Respond with ONLY the numbered list of predicates, nothing else.\"\"\"\n",
    "\n",
    "    print(f\"Sending batch request for {empty_count} predicates...\")\n",
    "    \n",
    "    llm_filled_count = 0\n",
    "    fallback_count = 0\n",
    "    successful_requests = 0\n",
    "    failed_requests = 0\n",
    "    \n",
    "    try:\n",
    "        # Single API request for all missing predicates\n",
    "        response = model.generate_content(\n",
    "            batch_prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                max_output_tokens=empty_count * 10,  # Adjust based on number of predicates\n",
    "                temperature=0.3,\n",
    "                candidate_count=1\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        response_text = response.text.strip()\n",
    "        print(\"✓ Batch API request successful\")\n",
    "        successful_requests = 1\n",
    "        \n",
    "        # Parse the response to extract individual predicates\n",
    "        response_lines = response_text.split('\\n')\n",
    "        \n",
    "        # Use regex to extract numbered responses\n",
    "        predicate_suggestions = {}\n",
    "        for line in response_lines:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                # Match patterns like \"1. biolink:treats\" or \"1) biolink:treats\" or \"1 biolink:treats\"\n",
    "                match = re.match(r'^(\\d+)[\\.\\)\\s]+(.+)', line)\n",
    "                if match:\n",
    "                    case_num = int(match.group(1))\n",
    "                    suggested_predicate = match.group(2).strip()\n",
    "                    \n",
    "                    # Clean up the suggestion (remove quotes, extra text)\n",
    "                    suggested_predicate = suggested_predicate.replace('\"', '').replace(\"'\", \"\")\n",
    "                    \n",
    "                    # Try exact match first\n",
    "                    if suggested_predicate in unique_predicates:\n",
    "                        predicate_suggestions[case_num] = suggested_predicate\n",
    "                    else:\n",
    "                        # Try partial matching\n",
    "                        for predicate in unique_predicates:\n",
    "                            if predicate in suggested_predicate or suggested_predicate in predicate:\n",
    "                                predicate_suggestions[case_num] = predicate\n",
    "                                break\n",
    "        \n",
    "        print(f\"✓ Successfully parsed {len(predicate_suggestions)} predicates from response\")\n",
    "        \n",
    "        # Apply the suggestions to the dataframe\n",
    "        for case_num, idx in case_mapping.items():\n",
    "            if case_num in predicate_suggestions:\n",
    "                suggested_predicate = predicate_suggestions[case_num]\n",
    "                df.at[idx, 'predicate'] = suggested_predicate\n",
    "                llm_filled_count += 1\n",
    "                print(f\"✓ Row {idx}: Filled with '{suggested_predicate}'\")\n",
    "            else:\n",
    "                # Fallback to random selection\n",
    "                fallback_predicate = random.choice(unique_predicates)\n",
    "                df.at[idx, 'predicate'] = fallback_predicate\n",
    "                fallback_count += 1\n",
    "                print(f\"⚠ Row {idx}: No suggestion found, used random '{fallback_predicate}'\")\n",
    "        \n",
    "        # Estimate token usage\n",
    "        input_tokens = len(batch_prompt.split()) * 1.3\n",
    "        output_tokens = len(response_text.split()) * 1.3\n",
    "        \n",
    "        # Store response details\n",
    "        responses = [{\n",
    "            'batch_request': True,\n",
    "            'total_cases': empty_count,\n",
    "            'prompt': batch_prompt,\n",
    "            'response_text': response_text,\n",
    "            'parsed_suggestions': predicate_suggestions,\n",
    "            'case_mapping': case_mapping,\n",
    "            'success': True,\n",
    "            'estimated_input_tokens': input_tokens,\n",
    "            'estimated_output_tokens': output_tokens\n",
    "        }]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Batch API request failed: {e}\")\n",
    "        failed_requests = 1\n",
    "        \n",
    "        # Fallback: fill all with random predicates\n",
    "        for idx in empty_indices:\n",
    "            fallback_predicate = random.choice(unique_predicates)\n",
    "            df.at[idx, 'predicate'] = fallback_predicate\n",
    "            fallback_count += 1\n",
    "        \n",
    "        responses = [{\n",
    "            'batch_request': True,\n",
    "            'total_cases': empty_count,\n",
    "            'prompt': batch_prompt,\n",
    "            'error': str(e),\n",
    "            'success': False,\n",
    "            'fallback_used': True\n",
    "        }]\n",
    "        \n",
    "        input_tokens = len(batch_prompt.split()) * 1.3\n",
    "        output_tokens = 0\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    # Create metrics summary\n",
    "    metrics = {\n",
    "        'total_empty_predicates': empty_count,\n",
    "        'llm_filled_count': llm_filled_count,\n",
    "        'fallback_count': fallback_count,\n",
    "        'successful_requests': successful_requests,\n",
    "        'failed_requests': failed_requests,\n",
    "        'total_requests': successful_requests + failed_requests,\n",
    "        'success_rate': successful_requests / (successful_requests + failed_requests) if (successful_requests + failed_requests) > 0 else 0,\n",
    "        'llm_success_rate': llm_filled_count / empty_count if empty_count > 0 else 0,\n",
    "        'total_processing_time_seconds': total_time,\n",
    "        'estimated_total_input_tokens': input_tokens,\n",
    "        'estimated_total_output_tokens': output_tokens,\n",
    "        'estimated_total_tokens': input_tokens + output_tokens,\n",
    "        'batch_processing': True,\n",
    "        'speed_improvement': f\"~{empty_count}x faster than individual requests\"\n",
    "    }\n",
    "    \n",
    "    # Save files\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    with open(metrics_file, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    with open(responses_file, 'w') as f:\n",
    "        json.dump(responses, f, indent=2)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n=== Batch LLM Processing Complete ===\")\n",
    "    print(f\"LLM filled predicates saved to: {output_file}\")\n",
    "    print(f\"Total predicates filled by LLM: {llm_filled_count}/{empty_count}\")\n",
    "    print(f\"Fallback (random) assignments: {fallback_count}\")\n",
    "    print(f\"LLM success rate: {metrics['llm_success_rate']:.2%}\")\n",
    "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "    print(f\"Estimated tokens used: {int(input_tokens + output_tokens)}\")\n",
    "    print(f\"Speed improvement: ~{empty_count}x faster than individual requests!\")\n",
    "    print(f\"Metrics saved to: {metrics_file}\")\n",
    "    print(f\"Responses saved to: {responses_file}\")\n",
    "    \n",
    "    return df, metrics, responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a780b0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (100, 3)\n",
      "Index range: 0 to 99\n",
      "Found 42 empty predicates to fill\n",
      "Sending batch request for 42 predicates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759347252.225361  433971 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Batch API request successful\n",
      "✓ Successfully parsed 42 predicates from response\n",
      "✓ Row 1: Filled with 'biolink:treats'\n",
      "✓ Row 2: Filled with 'biolink:treats'\n",
      "✓ Row 3: Filled with 'biolink:regulates'\n",
      "✓ Row 6: Filled with 'biolink:subclass_of'\n",
      "✓ Row 8: Filled with 'biolink:regulates'\n",
      "✓ Row 9: Filled with 'biolink:subclass_of'\n",
      "✓ Row 16: Filled with 'biolink:occurs_in'\n",
      "✓ Row 18: Filled with 'biolink:affects'\n",
      "✓ Row 22: Filled with 'biolink:subclass_of'\n",
      "✓ Row 27: Filled with 'biolink:positively_correlated_with'\n",
      "✓ Row 32: Filled with 'biolink:occurs_in'\n",
      "✓ Row 37: Filled with 'biolink:occurs_in'\n",
      "✓ Row 38: Filled with 'biolink:subclass_of'\n",
      "✓ Row 40: Filled with 'biolink:occurs_in'\n",
      "✓ Row 41: Filled with 'biolink:correlated_with'\n",
      "✓ Row 43: Filled with 'biolink:has_part'\n",
      "✓ Row 45: Filled with 'biolink:occurs_in'\n",
      "✓ Row 46: Filled with 'biolink:affects'\n",
      "✓ Row 47: Filled with 'biolink:contributes_to'\n",
      "✓ Row 49: Filled with 'biolink:similar_to'\n",
      "✓ Row 52: Filled with 'biolink:similar_to'\n",
      "✓ Row 54: Filled with 'biolink:occurs_in'\n",
      "✓ Row 58: Filled with 'biolink:contributes_to'\n",
      "✓ Row 59: Filled with 'biolink:subclass_of'\n",
      "✓ Row 60: Filled with 'biolink:has_participant'\n",
      "✓ Row 62: Filled with 'biolink:regulates'\n",
      "✓ Row 63: Filled with 'biolink:similar_to'\n",
      "✓ Row 64: Filled with 'biolink:genetically_associated_with'\n",
      "✓ Row 66: Filled with 'biolink:occurs_in'\n",
      "✓ Row 67: Filled with 'biolink:contributes_to'\n",
      "✓ Row 68: Filled with 'biolink:occurs_in'\n",
      "✓ Row 72: Filled with 'biolink:causes'\n",
      "✓ Row 73: Filled with 'biolink:regulates'\n",
      "✓ Row 75: Filled with 'biolink:correlated_with'\n",
      "✓ Row 77: Filled with 'biolink:regulates'\n",
      "✓ Row 78: Filled with 'biolink:has_part'\n",
      "✓ Row 81: Filled with 'biolink:has_part'\n",
      "✓ Row 82: Filled with 'biolink:has_participant'\n",
      "✓ Row 85: Filled with 'biolink:correlated_with'\n",
      "✓ Row 89: Filled with 'biolink:negatively_correlated_with'\n",
      "✓ Row 94: Filled with 'biolink:capable_of'\n",
      "✓ Row 96: Filled with 'biolink:contributes_to'\n",
      "\n",
      "=== Batch LLM Processing Complete ===\n",
      "LLM filled predicates saved to: gemini_filled_test.csv\n",
      "Total predicates filled by LLM: 42/42\n",
      "Fallback (random) assignments: 0\n",
      "LLM success rate: 100.00%\n",
      "Total processing time: 60.83 seconds\n",
      "Estimated tokens used: 544\n",
      "Speed improvement: ~42x faster than individual requests!\n",
      "Metrics saved to: gemini_metrics_test.json\n",
      "Responses saved to: gemini_responses_test.json\n"
     ]
    }
   ],
   "source": [
    "# Test the LLM function with a subset of data first - Fix the indexing issue\n",
    "test_df = modified.copy().reset_index(drop=True)  # Reset index to 0, 1, 2, 3...\n",
    "\n",
    "print(f\"DataFrame shape: {test_df.shape}\")\n",
    "print(f\"Index range: {test_df.index.min()} to {test_df.index.max()}\")\n",
    "\n",
    "filled_df, metrics, responses = fill_missing_predicates_llm_base(\n",
    "    test_df,\n",
    "    unique_predicates,\n",
    "    output_file='gemini_filled_test.csv',\n",
    "    metrics_file='gemini_metrics_test.json',\n",
    "    responses_file='gemini_responses_test.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81942dca",
   "metadata": {},
   "source": [
    "# in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a962d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import gzip\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# URLs for the .gz files (tool descriptions)\n",
    "urls = [\n",
    "    \"https://ftp.ncbi.nlm.nih.gov/pub/lu/PubTator3/bioconcepts2pubtator3.gz\",\n",
    "    \"https://ftp.ncbi.nlm.nih.gov/pub/lu/PubTator3/gene2pubtator3.gz\"\n",
    "]\n",
    "\n",
    "local_files = [\"bioconcepts2pubtator3.gz\", \"gene2pubtator3.gz\"]\n",
    "\n",
    "# Function to download the .gz files from URLs\n",
    "def download_file(url, local_path):\n",
    "    response = requests.get(url)\n",
    "    with open(local_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded file: {local_path}\")\n",
    "\n",
    "# Download the files\n",
    "# for url, local_file in zip(urls, local_files):\n",
    "#     download_file(url, local_file)\n",
    "\n",
    "# Initialize ChromaDB client (new method)\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Create or get the collection\n",
    "collection = client.create_collection(\"pubtator_data\")\n",
    "\n",
    "# Load the transformer model for embedding\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Initialize text splitter (chunk text into smaller pieces)\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "def embed_text(text):\n",
    "    \"\"\"Embed text into vector using pre-trained model.\"\"\"\n",
    "    embedding = model.encode(text)\n",
    "    return embedding\n",
    "\n",
    "# Function to read and process the .gz files\n",
    "def process_gz_file(file_path):\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        return f.readlines()\n",
    "\n",
    "# Process and insert the data into ChromaDB\n",
    "for file in local_files:\n",
    "    print(f\"Processing file: {file}\")\n",
    "    lines = process_gz_file(file)\n",
    "    \n",
    "    # Split lines into smaller chunks\n",
    "    for line in lines:\n",
    "        chunks = splitter.split_text(line.strip())  # Split long descriptions into chunks\n",
    "        \n",
    "        # For each chunk, generate embedding and store in ChromaDB\n",
    "        for chunk in chunks:\n",
    "            embedding = embed_text(chunk)\n",
    "            \n",
    "            # Add document and metadata (such as source file and chunk position) to ChromaDB\n",
    "            collection.add(\n",
    "                documents=[chunk],\n",
    "                metadatas=[{\"source\": file}],\n",
    "                embeddings=[embedding]\n",
    "            )\n",
    "\n",
    "print(\"Data inserted into ChromaDB.\")\n",
    "\n",
    "# Query ChromaDB for relevant documents (example query)\n",
    "query = \"What is gene expression in biological research?\"\n",
    "query_embedding = embed_text(query)\n",
    "\n",
    "# Perform the retrieval to get top-k similar documents\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=5  # Retrieve top 5 similar documents\n",
    ")\n",
    "\n",
    "print(\"Retrieved documents:\")\n",
    "for doc, metadata in zip(results[\"documents\"], results[\"metadatas\"]):\n",
    "    print(f\"Document: {doc} (Source: {metadata['source']})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
